<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Orange Pi Hadoop Cluster : Create a Hadoop 2.7.3 Cluster with Orange Pi One">

    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=ba51bbc11b40b1112245c08a5e4cdc58efdec041">

    <title>Orange Pi Hadoop Cluster</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="http://github.com/andrew-pyle/Single-Board-Cluster">View on GitHub</a>

          <h1 id="project_title">Orange Pi Hadoop Cluster</h1>
          <h2 id="project_tagline">
            <!-- Create a Hadoop 2.7.3 Cluster with Orange Pi One<br /> -->
            Andrew Pyle | IFSC 7370 | April 2017
          </h2>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <!-- <div class="container">
          <img src="/images/orange-cover.JPG" alt="Orange Pi One Board" />
          <div class="overlay">
            <h1>Orange Pi One & Apache Hadoop</h1>
            <img src="images/hadoop-logo.JPG" alt="Orange Pi One" />
            <h1><span>+<span class="spacer"></span><br /><span class="spacer"></span>Orange Pi One</span></h1>
          </div>
        </div> -->
        <p><img src="images/3MT-Andrew-Pyle.png" alt="Orange Pi One" /></p>

<h1 id="small-scale-big-data">Small Scale, Big Data</h1>
<p><a href="http://hadoop.apache.org/">Hadoop</a> is an open-source distributed computing framework from the <a href="http://www.apache.org/">Apache Software Foundation</a>. It is primarily aimed at Big Data analytics and has become the de-facto standard for Distributed analytics. It runs on Linux, and (lucky for us students of data) the <a href="http://linux-sunxi.org/Orange_Pi_One">Orange Pi One</a> is a very affordable machine to learn about distributed computing, albeit with very limited resources.</p>

<p>So let’s set up a fully distributed, physical Hadoop 2 cluster with single-board Linux computers! I will be using two <a href="http://linux-sunxi.org/Orange_Pi_One">Orange Pi One</a> boards. By the end of this journal, we will have a ethernet connected Hadoop 2 cluster of two single-board computers. This setup will not be powerful enough for production use, but it will demonstrate a simple implementation of the Hadoop 2 ecosystem for learning purposes.</p>

<h1 id="contents">Contents</h1>
<!-- MDTOC maxdepth:2 firsth1:0 numbering:1 flatten:0 bullets:0 updateOnSave:1 -->

<ol>
  <li><a href="#materials">Materials</a></li>
  <li><a href="#orange-pi-setup">Orange Pi Setup</a> <br />
 2.1. <a href="#install-the-operating-system">Install the Operating System</a> <br />
 2.2. <a href="#operate-the-operating-system">Operate the Operating System</a></li>
  <li><a href="#hadoop-273">Hadoop 2.7.3</a> <br />
 3.1. <a href="#hadoop-2-architecture">Hadoop 2 Architecture</a> <br />
 3.2. <a href="#install-oracle-java">Install Oracle Java</a> <br />
 3.3. <a href="#create-hadoop-user">Create Hadoop user</a> <br />
 3.4. <a href="#install-hadoop-273">Install Hadoop 2.7.3</a> <br />
 3.5. <a href="#connect-hadoop-cluster">Connect Hadoop Cluster</a> <br />
 3.6. <a href="#hadoop-configuration">Hadoop Configuration</a> <br />
 3.7. <a href="#create-hadoop-nodes">Create Hadoop Nodes</a> <br />
 3.8. <a href="#start-hadoop">Start Hadoop</a> <br />
 3.9. <a href="#test-hadoop">Test Hadoop</a></li>
  <li><a href="#troubleshooting-hadoop">Troubleshooting Hadoop</a> <br />
 4.1. <a href="#ensure-hostnames-working-properly">Ensure Hostnames Working Properly</a> <br />
 4.2. <a href="#check-the-java-version">Check the Java version</a> <br />
 4.3. <a href="#check-hostnames">Check Hostnames</a> <br />
 4.4. <a href="#ssh-access">SSH Access</a> <br />
 4.5. <a href="#hadoop-install-verification">Hadoop Install Verification</a> <br />
 4.6. <a href="#hdfs-fsck">HDFS <code class="highlighter-rouge">fsck</code></a> <br />
 4.7. <a href="#reformat-hdfs">Reformat HDFS</a> <br />
 4.8. <a href="#hadoop-configuration">Hadoop Configuration</a> <br />
 4.9. <a href="#word-count-test">Word Count Test</a> <br />
 4.10. <a href="#calculate-π-test">Calculate π Test</a> <br />
 4.11. <a href="#optimize-configuration">Optimize Configuration</a> <br />
 4.12. <a href="#test-again">Test Again</a></li>
  <li><a href="#project-discussion">Project Discussion</a> <br />
 5.1. <a href="#hadoop-application-failure-analysis">Hadoop Application Failure Analysis</a> <br />
 5.2. <a href="#future-work">Future Work</a> <br />
 5.3. <a href="#conclusion">Conclusion</a></li>
</ol>

<!-- /MDTOC -->

<h2 id="materials">Materials</h2>
<p>I am using the following items to create the cluster:</p>

<ul>
  <li>2 <a href="http://linux-sunxi.org/Orange_Pi_One">Orange Pi Ones</a></li>
  <li>For each single-board computer
    <ul>
      <li>Sandisk Ultra 16 GB Class 10 micro SD card (<a href="https://www.amazon.com/gp/product/B010Q57SEE/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;psc=1">Amazon</a>)
(<a href="https://kb.sandisk.com/app/answers/detail/a_id/1996/~/difference-between-speed-class%2C-uhs-speed-class%2C-and-speed-ratings">Speed Ratings Info</a>)</li>
      <li>5V 2A USB Power supply</li>
      <li>USB-DC barrel plug cable (<a href="http://a.co/dzPdzaR">Amazon</a>)</li>
    </ul>
  </li>
  <li>Network Interface
    <ul>
      <li>Ethernet switch and CAT5 cable
<em>Note: USB WiFi Dongles could work, but are less reliable</em></li>
    </ul>
  </li>
  <li>Display and HDMI-DVI cable</li>
</ul>

<h2 id="orange-pi-setup">Orange Pi Setup</h2>

<h3 id="install-the-operating-system">Install the Operating System</h3>
<p>I purchased 2 boards from <a href="https://www.aliexpress.com/item/Orange-Pi-One-H3-Quad-core-Support-ubuntu-linux-and-android-mini-PC-Beyond-Raspberry-Pi/32603308880.html">AliExpress.com</a>, along with a Sandisk Ultra 16 GB Class 10 micro SD card from <a href="https://www.amazon.com/gp/product/B010Q57SEE/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;psc=1">Amazon</a> for each.</p>

<p><img src="/images/orange-top.JPG" alt="Orange Pi One" /></p>

<p>Each Orange Pi One requires a 5V 2A power supply with a DC barrel plug (<a href="http://linux-sunxi.org/Orange_Pi_One">4.0mm/1.7mm - center positive</a>). I used a <a href="http://a.co/dzPdzaR">USB-DC barrel plug cable from Amazon</a> so I could use a USB power supply that I had lying around the house.</p>

<p><img src="/images/orange-cable.JPG" alt="USB-DC Barrel Plug Cable" /></p>

<h4 id="sd-card-integrity">SD Card Integrity</h4>
<p>First, we need to ensure the integrity of the SD cards.</p>
<blockquote>
  <p>Many scam artists try to market small flash disks as large ones by changing the appearance of the disk. We will use a software test to verify the size of the disk we have. I am using <a href="http://oss.digirati.com.br/f3/">F3 by Digirati</a> (GPL v3). It runs on Windows, OS X, Linux, and other operating systems. The website includes a very straightforward instructions regarding it’s use.</p>
</blockquote>

<p><em>(Note: I am using MacOS Sierra for the following commands, so if you are using a non-Unix-based OS, note that your commands will be different.)</em></p>

<p>Download and unzip the program, then compile it for your system.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd </span>f3-6.0

<span class="gp">$ </span>ls
LICENSE		f3fix.c		f3write.h2w	libutils.c	version.h
Makefile	f3probe.c	libdevs.c	libutils.h
README.md	f3read.1	libdevs.h	log-f3wr
changelog	f3read.c	libprobe.c	utils.c
f3brew.c	f3write.c	libprobe.h	utils.h

<span class="gp">$ </span>make
cc -std<span class="o">=</span>c99 -Wall -Wextra -pedantic -MMD -ggdb   -c -o utils.o utils.c
cc -std<span class="o">=</span>c99 -Wall -Wextra -pedantic -MMD -ggdb   -c -o f3write.o f3write.c
cc -o f3write utils.o f3write.o -lm
cc -std<span class="o">=</span>c99 -Wall -Wextra -pedantic -MMD -ggdb   -c -o f3read.o f3read.c
cc -o f3read utils.o f3read.o
</code></pre>
</div>

<p>F3 uses two commands to verify the flash memory: f3write and f3read. The f3write command writes blocks of data to the disk, and f3read reads the data written. In our case, the output will show if the card’s claimed capacity matches its usable capacity.</p>

<p>Run f3write on the disk’s location in the file system. This can take a while. (Note: if you are using macOS as I am, the location is at /Volumes/your_disk by default.)</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>./f3write /Volumes/<span class="s2">"NO NAME"</span>
Free space: 14.83 GB
Creating file 1.h2w ... OK!                         
Creating file 2.h2w ... OK!                         
Creating file 3.h2w ... OK!                         
Creating file 4.h2w ... OK!                         
Creating file 5.h2w ... OK!                         
Creating file 6.h2w ... OK!                         
Creating file 7.h2w ... OK!                         
Creating file 8.h2w ... OK!                         
Creating file 9.h2w ... OK!                         
Creating file 10.h2w ... OK!                         
Creating file 11.h2w ... OK!                        
Creating file 12.h2w ... OK!                        
Creating file 13.h2w ... OK!                        
Creating file 14.h2w ... OK!                        
Creating file 15.h2w ... OK!                        
Free space: 0.00 Byte
Average writing speed: 9.79 MB/s
</code></pre>
</div>

<blockquote>
  <p><em>Note: I received the following error when executing f3write the first time. I just retried the f3write command, and execution was successful. I am not sure what caused the error, and a quick Google search did not return any help. I will just keep the info in mind.</em></p>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>./f3write /Volumes/<span class="s2">"NO NAME"</span>
Free space: 14.83 GB
Creating file 1.h2w ... 0.29% -- 10.00 MB/s

f3write: Write to file /Volumes/NO NAME/1.h2w failed: Input/output error
</code></pre>
  </div>
</blockquote>

<p>And run f3read to verify the data written.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>./f3read /Volumes/<span class="s2">"NO NAME"</span>
                  SECTORS      ok/corrupted/changed/overwritten
Validating file 1.h2w ... 2097152/        0/      0/      0
Validating file 2.h2w ... 2097152/        0/      0/      0
Validating file 3.h2w ... 2097152/        0/      0/      0
Validating file 4.h2w ... 2097152/        0/      0/      0
Validating file 5.h2w ... 2097152/        0/      0/      0
Validating file 6.h2w ... 2097152/        0/      0/      0
Validating file 7.h2w ... 2097152/        0/      0/      0
Validating file 8.h2w ... 2097152/        0/      0/      0
Validating file 9.h2w ... 2097152/        0/      0/      0
Validating file 10.h2w ... 2097152/        0/      0/      0
Validating file 11.h2w ... 2097152/        0/      0/      0
Validating file 12.h2w ... 2097152/        0/      0/      0
Validating file 13.h2w ... 2097152/        0/      0/      0
Validating file 14.h2w ... 2097152/        0/      0/      0
Validating file 15.h2w ... 1733888/        0/      0/      0

  Data OK: 14.83 GB <span class="o">(</span>31094016 sectors<span class="o">)</span>
Data LOST: 0.00 Byte <span class="o">(</span>0 sectors<span class="o">)</span>
	       Corrupted: 0.00 Byte <span class="o">(</span>0 sectors<span class="o">)</span>
	Slightly changed: 0.00 Byte <span class="o">(</span>0 sectors<span class="o">)</span>
	     Overwritten: 0.00 Byte <span class="o">(</span>0 sectors<span class="o">)</span>
Average reading speed: 22.27 MB/s
</code></pre>
</div>
<p>Great! no files are listed in the ‘corrupted’ column above, so the microSD cards should be good.</p>

<p>If your disk shows files in the ‘corrupted’ category or a much smaller capacity than advertised, ask for your money back. See F3’s website for more specifics on the program’s use and functionality.</p>

<h4 id="flash-the-os">Flash the OS</h4>
<p>The next step is to write the operating system for the orange pi to a microSD card, called “flashing” the card.</p>

<p>We will be installing the <a href="https://www.armbian.com/orange-pi-one/">Armbian distribution</a> of Debian Jessie. It is a solely CLI image, which means that it uses a text-based command-line interface. They host a Ubuntu Xenial image with CLI only and GUI desktop interface versions as well.</p>

<blockquote>
  <p>Shenzhen Xunlong CO., Limited, the makers of the Orange Pi, host their own <a href="http://www.orangepi.org/downloadresources/">OS images</a> for the Orange Pi One, but they are the same images as for another model, the Orange Pi PC. I have read that the image is not fully supported by the hardware of the One, while Armbian’s image is customized for the One. <a href="http://linux-sunxi.org/Xunlong_Orange_Pi_One_%26_Lite">See linux-sunxi</a> for details.</p>
</blockquote>

<p>Download and unzip the OS image. I used <a href="http://unarchiver.c3.cx/unarchiver">The Unarchiver</a> for mac to unzip the .7z file. The OS image file is highlighted.
<img src="./images/armbian-unzip.png" alt="alt text" title="Unzipped Armbian Orange Pi One OS image" />
Armbian offers a <a href="https://docs.armbian.com/User-Guide_Getting-Started/">GPG signature</a> for authentication of the OS image download, if you want to check that out. The <a href="http://notes.jerzygangi.com/the-best-pgp-tutorial-for-mac-os-x-ever/">process</a> is quite involved however.</p>

<p>We will be using <a href="https://etcher.io">Etcher</a> to flash the microSD card. It is a simple interface available for many OSes.</p>

<p><img src="/images/etcher.png" alt="Etcher Screenshot" /></p>

<p>Just choose the Armbian image and the microSD card. The card was auto selected for me. My Armbian image was named as follows. Ensure that you have the correct filename.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>Armbian_5.25_Orangepione_Debian_jessie_default_3.4.113
</code></pre>
</div>

<p>Etcher gave a success message, so we can now insert the microSD card into the Orange Pi and begin the first boot.</p>

<p><img src="/images/etcher-success.png" alt="Etcher Success Screenshot" /></p>

<p>Now, just load the microSD into the Orange Pi board, connect a USB keyboard and display, and connect power. I’m using a HDMI-DVI connection.</p>

<h4 id="problem">Problem!</h4>
<p>At this point, we should see a red power light and the Pi should boot up. However, I see no lights at all. This is a problem!</p>

<p>After measuring some voltages with a multimeter, it seems that the issue is the USB-DC power cable. I can measure 5V from the power supply, but no voltage from the cable connector. I will get another cable and try to boot from the Armbian image again.</p>

<h4 id="new-power-cables">New Power Cables</h4>
<p>Two new cables from Amazon (<a href="http://a.co/gDyAZoq">this one</a> and <a href="http://a.co/ftKETZe">this one</a>) arrived today. I will try to boot the Armbian image again. I am skeptical that this will work, since it seems like a cable is the least likely piece of the computer system to fail.</p>

<p>Let’s connect the Orange Pi as above, and try to boot!</p>

<h4 id="boot-up">Boot Up</h4>
<p>LIGHTS! It does seem as if the cable was bad. (I’ll try to get my money back.) I am getting a green light and blinking red light. The DVI monitor showed an error message and then a blank screen.</p>
<blockquote>
  <p>I wish I’d gotten a video here of the light sequence during the boot process, but I didn’t have the camera ready.</p>
</blockquote>

<p>This all means the resolution coming from the Orange Pi is unusable by the monitor. This also means that I currently have no idea what is going on with the Orange Pi Boot process. A quick Google search didn’t turn up anything specific, so I will try to boot with another power supply.</p>

<blockquote>
  <p>The Orange Pi is extremely picky about the power supply. Even a supply marked 5V 2A will fail frequently. It seems that the voltage must be extremely stable over all amperages, and the amperage must not fall far below 2A at any time during operation. It seems that the best solution for the Orange Pi power supply is to buy the  5V 3A power supply from <a href="https://www.aliexpress.com/store/product/orange-pi-orange-pi-plus-Power-Adapter-5V-2A-Power-Supply-Micro-USB-Charger/1553371_32248555041.html">AliExpress</a>. They have both US and European versions.</p>
</blockquote>

<p>I use the <a href="http://plugable.com/products/USB2-HUB4BC">Plugable 4-port USB Hub</a> to power my Raspberry Pi with no issues, so I will give that a try as the power supply.</p>

<p><img src="/images/orange-power.JPG" alt="Orange Pi boot trial with Plugable USB Hub" /></p>

<p>It works! I got a brief look at the boot process before the monitor went blank due to resolution issues (probably), and the green light went solid with occasional flashes (the light is on the lower right corner of the board, by the GPIO pins). The red light started solid and then went out.</p>

<hr />

<h3 id="operate-the-operating-system">Operate the Operating System</h3>
<p>Now that the boot process has begun successfully, let’s login and configure the opartating system.</p>

<h4 id="login-with-secure-shell-ssh">Login with Secure Shell (SSH)</h4>
<p>Since the monitor is blank, it’s not helping us complete the initial boot process. Thankfully SSH is enabled by default on the Orange Pi! SSH is a method of remotely logging in to a Unix-based (e.g. Linux) operating system. <a href="https://en.wikipedia.org/wiki/Secure_Shell">SSH Wikipedia link</a>.</p>

<p>Since the Orange Pi booted up while connected to the network, it received an IP Address by DHCP. I can check that at my router’s homepage.</p>
<blockquote>
  <p>(There are other methods to ascertain the IP address of the devices on your home network, like <a href="https://www.fing.io/">Fing</a> for iOS and Android).</p>
</blockquote>

<p>SSH into the Orange Pi One. The default password for root on Armbian is ‘1234’. Add the RSA fingerprint to the list of known hosts if asked. This registers the Orange Pi hardware with your computer.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ssh root@[ip_address]   <span class="c"># Replace [ip_address] with the IP</span>
                          <span class="c"># address of your Orange Pi</span>

root@192.168.0.108 password:    <span class="c">#1234</span>
</code></pre>
</div>
<p>A nice login screen is displayed upon SSH connection.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>  ___                               ____  _    ___             
 / _ \ _ __ __ _ _ __   __ _  ___  |  _ \(_)  / _ \ _ __   ___
| | | | '__/ _` | '_ \ / _` |/ _ \ | |_) | | | | | | '_ \ / _ \
| |_| | | | (_| | | | | (_| |  __/ |  __/| | | |_| | | | |  __/
 \___/|_|  \__,_|_| |_|\__, |\___| |_|   |_|  \___/|_| |_|\___|
                       |___/                                   

Welcome to ARMBIAN 5.25 stable Debian GNU/Linux 8 (jessie) 3.4.113-sun8i   
System load:   0.01            	Up time:       11 min		
Memory usage:  10 % of 494Mb  	IP:            192.168.0.108
CPU temp:      39°C           	
Usage of /:    13% of 15G    	

</code></pre>
</div>

<h4 id="configure-system">Configure System</h4>
<p>Now that we can boot the Orange Pi up, lets get the system configured for use.</p>

<p>Follow the initial boot prompts from Armbian, like changing the root password, making a new user account, and setting the display settings.</p>

<p>The h3disp program is provided to set the display settings.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@orangepione:$ sudo h3disp
Usage: h3disp [-h/-H] -m [video mode] [-d] [-c [0-2]]

############################################################################

 This is a tool to set the display resolution of your Orange
 Pi by patching script.bin.

 In case you use an HDMI-to-DVI converter please use the -d switch.

 The resolution can be set using the -m switch. The following resolutions
 are currently supported:

    480i    use "-m 480i" or "-m 0"
    576i	use "-m 576i" or "-m 1"
    480p	use "-m 480p" or "-m 2"
    576p	use "-m 576p" or "-m 3"
    720p50	use "-m 720p50" or "-m 4"
    720p60	use "-m 720p60" or "-m 5"
    1080i50	use "-m 1080i50" or "-m 6"
    1080i60	use "-m 1080i60" or "-m 7"
    1080p24	use "-m 1080p24" or "-m 8"
    1080p50	use "-m 1080p50" or "-m 9"
    1080p60	use "-m 1080p60" or "-m 10"
    1080p25	use "-m 1080p25" or "-m 11"
    1080p30	use "-m 1080p30" or "-m 12"
    1080p24_3d	use "-m 1080p24_3d" or "-m 13"
    720p50_3d	use "-m 720p50_3d" or "-m 14"
    720p60_3d	use "-m 720p60_3d" or "-m 15"
    1080p24_3d	use "-m 1080p24_3d" or "-m 23"
    720p50_3d	use "-m 720p50_3d" or "-m 24"
    720p60_3d	use "-m 720p60_3d" or "-m 25"
    1080p25	use "-m 1080p25" or "-m 26"
    1080p30	use "-m 1080p30" or "-m 27"
    4kp30	use "-m 4kp30" or "-m 28"
    4kp25	use "-m 4kp25" or "-m 29"
    800x480	use "-m 800x480" or "-m 31"
    1024x768	use "-m 1024x768" or "-m 32"
    1280x1024	use "-m 1280x1024" or "-m 33"
    1360x768	use "-m 1360x768" or "-m 34"
    1440x900	use "-m 1440x900" or "-m 35"
    1680x1050	use "-m 1680x1050" or "-m 36"

 Two examples:

'h3disp -m 1080p60 -d' (1920x1080@60Hz DVI)
    'h3disp -m 720i' (1280x720@30Hz HDMI)

 You can also specify the colour-range for your HDMI-display with the -c switch.

 The following values for -c are currently supported:

0 -- RGB range 16-255 (Default, use "-c 0")
    1 -- RGB range 0-255 (Full range, use "-c 1")
    2 -- RGB range 16-235 (Limited video, "-c 2")

############################################################################
</code></pre>
</div>
<p>My monitor is 1280x1024 @ 60 hz with a DVI adapter. I am not sure why the DVI adapter makes a difference, but it has its own ‘-d’ flag. I will use the following command:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">root@orangepione:$ </span>h3disp -m 1280x1024 -d
</code></pre>
</div>
<p>After rebooting, the monitor shows the correct image, but it still shows a resoloution error message. After trying several other formats (all worse outcome than the above setting), I found that if I push the menu button on the monitor before it goes blank, it will interrupt the sleep timeout process, and after closing the menu, the error is gone and the monitor works properly. I can’t explain this one, but it seems to work!</p>

<h4 id="filesystem-resizing">Filesystem Resizing</h4>
<blockquote>
  <p>I am switching to the non-root user account I created in the installation process. It is always better to operate a CLI OS as a non-root user. It is too easy to unintentionally make a mistake as root, since root has so much power in the OS.</p>
</blockquote>

<p>When we flashed the Armbian OS image onto the microSD card, the image wasn’t nearly as large as the whole card’s storage capacity. If it were, we’d have a problem actually running the OS! But we do want the Orange Pi’s file system to have access to all the storage capacity on the microSD card once we’ve booted into the OS. Luckily, Armbian automatically resizes the filesystem to use the entire capacity of the card it’s installed on, leaving ~1% as free space.</p>

<p>Run the following commands to check.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">asp@orangepione:$ </span>fdisk -l

Disk /dev/mmcblk0: 14.9 GiB, 15931539456 bytes, 31116288 sectors
Units: sectors of 1 <span class="k">*</span> 512 <span class="o">=</span> 512 bytes
Sector size <span class="o">(</span>logical/physical<span class="o">)</span>: 512 bytes / 512 bytes
I/O size <span class="o">(</span>minimum/optimal<span class="o">)</span>: 512 bytes / 512 bytes
Disklabel <span class="nb">type</span>: dos
Disk identifier: 0x9781060c

Device         Boot Start      End  Sectors  Size Id Type
/dev/mmcblk0p1       2048 30493951 30491904 14.6G 83 Linux
</code></pre>
</div>
<p>Notice that the size of the only partition (/dev/mmcblk0p1) is close to the entire size of the microSD card, which is 16 GB in my case.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">asp@orangepione:$ </span>df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/mmcblk0p1   15G  1.1G   14G   8% /
udev             10M     0   10M   0% /dev
tmpfs            99M  4.5M   95M   5% /run
tmpfs           248M     0  248M   0% /dev/shm
tmpfs           5.0M  4.0K  5.0M   1% /run/lock
tmpfs           248M     0  248M   0% /sys/fs/cgroup
tmpfs           248M     0  248M   0% /tmp
log2ram          50M  748K   50M   2% /var/log
tmpfs            50M     0   50M   0% /run/user/1000
</code></pre>
</div>
<p>This output says that the filesystem (/dev/mmcblk0p1) has a size of 15 GB.</p>

<h4 id="set-timezone">Set Timezone</h4>
<p>Next let’s set the timezone.</p>
<blockquote>
  <p>I am switching back to root user here because the dpkg-reconfigure commands aren’t in the $PATH of my created user account, and I didn’t want to take the time to add them.</p>
</blockquote>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">root@orangepione:$ </span>dpkg-reconfigure tzdata
</code></pre>
</div>
<p><img src="/images/tzdata-america.png" alt="dpkg-reconfigure tzdata" /> <img src="/images/tzdata-chicago.png" alt="dpkg-reconfigure tzdata" /></p>
<div class="highlighter-rouge"><pre class="highlight"><code>Current default time zone: 'America/Chicago'
Local time is now:      Fri Mar 31 00:25:30 CDT 2017.
Universal Time is now:  Fri Mar 31 05:25:30 UTC 2017.
</code></pre>
</div>
<h4 id="run-apt-get-commands">Run <code class="highlighter-rouge">apt-get</code> Commands</h4>
<p>And lastly, I am getting a notification below the login screen that there are updates to install.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>  ___                               ____  _    ___             
 / _ \ _ __ __ _ _ __   __ _  ___  |  _ \(_)  / _ \ _ __   ___
| | | | '__/ _` | '_ \ / _` |/ _ \ | |_) | | | | | | '_ \ / _ \
| |_| | | | (_| | | | | (_| |  __/ |  __/| | | |_| | | | |  __/
 \___/|_|  \__,_|_| |_|\__, |\___| |_|   |_|  \___/|_| |_|\___|
                       |___/                                   

Welcome to ARMBIAN 5.25 stable Debian GNU/Linux 8 (jessie) 3.4.113-sun8i   
System load:   0.03            	Up time:       3 min		
Memory usage:  10 % of 494Mb  	IP:            192.168.0.108
CPU temp:      35°C           	
Usage of /:    7% of 15G   


[ 3 updates to install: apt-get upgrade ]
</code></pre>
</div>
<blockquote>
  <p>Linux Debian Note: apt-get is a package manager for the Debian OS. It handles installing programs on the CLI (which is a good thing, because I’d have trouble installing programs from the Internet without it!)
Run this command for more information.</p>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>man apt-get
</code></pre>
  </div>
  <p>Or see a plain-english description  <a href="http://askubuntu.com/questions/222348/what-does-sudo-apt-get-update-do">here</a>.</p>
</blockquote>

<p>We will run two commands to install the updates. This one udpates the existing program information. <strong>Be sure to run this one (update) before upgrade, or else you will be upgrading based on outdated package info.</strong></p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">root@orangepione:$ </span>apt-get update
</code></pre>
</div>
<p>And this one actually downloads and installs the new versions of the programs.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">root@orangepione:$ </span>apt-get upgrade
</code></pre>
</div>

<blockquote>
  <p>These commands can take a <strong>really</strong> long time to complete. Especially if it has not been run in a while. You are updating every single program on your computer at once! Be sure to wait until the command prompt appears again before typing new commands:</p>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">root@orangepione:$  </span>
</code></pre>
  </div>
</blockquote>

<p>After a successful upgrade, we are ready to begin the Hadoop 2 installation! Let’s pat ourselves on the back for a successful Orange Pi installation!</p>

<h3 id="references">References</h3>

<ol>
  <li><a href="http://linux-sunxi.org/Orange_Pi_One">Sunxi Orange Pi One Hardware Info</a></li>
  <li><a href="https://tdstraub.wordpress.com/2016/04/05/orange-pi-one-quick-start-guide/">Orange Pi One Quickstart Guide by Travis Straub</a></li>
  <li><a href="http://www.cnx-software.com/2016/03/16/orange-pi-one-board-quick-start-guide-with-armbian-debian-based-linux-distribution/">Orange Pi One Board Quick Start Guide with Armbian Debian based Linux Distribution</a></li>
  <li><a href="https://www.armbian.com/orange-pi-one/">Armbian - Orange Pi One</a></li>
  <li><a href="https://docs.armbian.com/User-Guide_Getting-Started/">Armbian Docs Guide</a></li>
  <li><a href="http://oss.digirati.com.br/f3/">F3 by Digirati</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Secure_Shell">SSH Wikipedia link</a></li>
</ol>

<h2 id="hadoop-273">Hadoop 2.7.3</h2>
<h3 id="hadoop-2-architecture">Hadoop 2 Architecture</h3>
<p>I didn’t know this starting out, but knowledge of the Hadoop architecture is really essential for understanding the configuration of a Hadoop cluster. It also really helps with troubleshooting your cluster, once Hadoop is installed and supposed to be working, but isn’t!</p>

<p>So let’s gain a quick overview of Hadoop 2 now. Read <a href="http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html">this article</a> for a more lengthy explanation.</p>

<p><img src="/images/yarn-architecture.gif" alt="Hadoop Cluster" />
<em>Image from <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop Documentation</a> © 2008-2017 Apache Software Foundation</em></p>

<h4 id="cluster-architecture">Cluster Architecture</h4>
<p>The Hadoop 2 architecture relies on pooling the resources of a group of computers linked by a network. Several programs work together to distribute programs and data among all the computers so that processing may be done in parallel.</p>

<p>This is a large improvement over the traditional data-processing paradigm, where a single machine stores all the data and does all the processing. Or that machine ships the data elsewhere for processing and ships the results back.</p>

<p>The Hadoop 2 architecture is composed of two types of computers:</p>
<ul>
  <li>Client</li>
  <li>Nodes</li>
</ul>

<p>Clients submit data to be stored in the cluster and programs to be run by cluster. Nodes are part of the cluster and are controlled by the Hadoop services. These services have access to the pooled memory, storage and processing power across the cluster.</p>

<h4 id="hadoop-services">Hadoop Services</h4>
<p>Hadoop manages the pooled resources of the nodes with the following services under the YARN framework in Hadoop 2:</p>
<ul>
  <li><code class="highlighter-rouge">NameNode</code>: Manages all the resources of the cluster</li>
  <li><code class="highlighter-rouge">SecondaryNameNode</code>: Creates snapshots of the cluster in case the NameNode crashes.</li>
  <li><code class="highlighter-rouge">ResourceManager</code>: Allocates resources for a container</li>
  <li><code class="highlighter-rouge">DataNode</code>: Stores and processes data</li>
  <li><code class="highlighter-rouge">NodeManager</code>: Reports each node’s resources to the <code class="highlighter-rouge">ResourceManager</code></li>
</ul>

<h4 id="job-execution">Job Execution</h4>
<p>When Hadoop cluster receives a job, it allocates a block of resources on each of several DataNodes. The job is called an Application, and receives an ID number. Each Application has multiple AppAttempts, in case one fails. AppAttempts can be restarted as well. The ApplicationMaster creates a container of the amount of resources dictated by the ResourceManager. Each AppAttempt is assigned to a container running on a DataNode. Each AppAttempt is made up of a number of Map and Reduce tasks, which are defined in the program submitted by the client. Hadoop clusters can only directly run programs written in a MapReduce paradigm.</p>

<p>Here are the important parts of a running Hadoop job:</p>
<ul>
  <li>Application</li>
  <li>ApplicationMaster</li>
  <li>Container</li>
  <li>AppAttempt</li>
  <li>Map Task</li>
  <li>Reduce Task</li>
</ul>

<p>The above information will become very relevant as we dive into the installation and configuration of the cluster! Let’s move on!</p>

<h4 id="references-1">References</h4>
<ol>
  <li><a href="http://www.apache.org/">Apache Software Foundation</a></li>
  <li><a href="http://hadoop.apache.org/">Apache Hadoop</a></li>
  <li><a href="https://hadoop.apache.org/docs/r2.7.3/">Apache Hadoop Documentation</a></li>
  <li><a href="http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html">Hadoop Architecture</a></li>
</ol>

<hr />

<h3 id="install-oracle-java">Install Oracle Java</h3>
<p><em>This section draws from the <a href="http://www.piprojects.xyz/install-hadoop-java-orange-pi/">Install Hadoop on the Orange Pi</a> tutorial from <a href="http://www.piprojects.xyz/">Pi Projects</a> for creating a single-node cluster. Check them out!</em></p>

<p>We will be installing <a href="http://hadoop.apache.org/releases.html">Apache Hadoop 2.7.3</a> (released 25 Aug 2016) on the Orange Pi. This is not the most up-to-date version, but there is more information on the Internet about Hadoop 2.7.x than either 2.8.x or 3.x.x.</p>

<p>Java is required for Hadoop, as much of the functionality of the system is rooted in executing Java code. Let’s install <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">Oracle Java JDK 8</a>. From what I’ve read online Oracle’s JDK is more performant for Hadoop clusters than Open JDK or other alternatives. See Apache info <a href="https://wiki.apache.org/hadoop/HadoopJavaVersions">here</a>.</p>

<p>I could not find a download path from Oracle to <code class="highlighter-rouge">wget</code> the tar archive directly to the Orange Pi, so I downloaded Oracle JDK for Linux ARM 32-bit processor onto my MacBook and transferred the tar archive to the Orange Pi with <code class="highlighter-rouge">scp</code>:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>scp filename username@destination_host:/path/to/file
</code></pre>
</div>
<p>On the Orange Pi, unpack the archive to the <code class="highlighter-rouge">/opt</code> directory with the <code class="highlighter-rouge">-C</code>flag:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>sudo tar xzvf jdk-8u121-linux-arm32-vfp-hflt.tar.gz -C /opt
</code></pre>
</div>
<p>Next, we will setup the Java installation with the following commands from <a href="http://www.piprojects.xyz/install-hadoop-java-orange-pi/">Pi Projects</a>. I do not entirely understand this process, but these commands set up Oracle Java 8 JDK as the default <code class="highlighter-rouge">java</code> and <code class="highlighter-rouge">javac</code> for our environment by creating <code class="highlighter-rouge">symlinks</code> from our installation to <code class="highlighter-rouge">/usr/bin/java</code> and <code class="highlighter-rouge">/usr/bin/java</code>.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo update-alternatives --install /usr/bin/javac javac /opt/jdk1.8.0_121/bin/javac 1

<span class="gp">$ </span>sudo update-alternatives --install /usr/bin/java java /opt/jdk1.8.0_121/bin/java 1

<span class="c"># Allows interactive selection of default java location</span>
<span class="gp">$ </span>sudo update-alternatives --config javac
<span class="gp">$ </span>sudo update-alternatives --config java
</code></pre>
</div>
<p>We can see that the symlinks have been made.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ls /usr/bin/java
/usr/bin/java
<span class="gp">asp@orangepione:~$ </span>ls -l  /usr/bin/ | grep java
lrwxrwxrwx 1 root root   22 Apr 14 20:17 java -&gt; /etc/alternatives/java
lrwxrwxrwx 1 root root   23 Apr 14 20:17 javac -&gt; /etc/alternatives/javac
</code></pre>
</div>
<h4 id="installing-java-references">Installing Java References</h4>
<ol>
  <li><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">Oracle Java JDK 8</a></li>
  <li><a href="http://www.rpiblog.com/2014/03/installing-oracle-jdk-8-on-raspberry-pi.html">Install JDK 8</a></li>
  <li><a href="https://wiki.apache.org/hadoop/HadoopJavaVersions">Hadoop Java Version Page</a></li>
</ol>

<hr />

<h3 id="create-hadoop-user">Create Hadoop user</h3>
<p>First, we will create a user for hadoop’s processes in the <code class="highlighter-rouge">sudo</code> group. This will allow the machines in the cluster to communicate with each other.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo addgroup hadoop
<span class="gp">$ </span>sudo adduser --ingroup hadoop hduser
<span class="gp">$ </span><span class="nv">$ </span>sudo adduser hduser sudo
</code></pre>
</div>

<hr />

<h3 id="install-hadoop-273">Install Hadoop 2.7.3</h3>
<h4 id="download-hadoop-273">Download Hadoop 2.7.3</h4>
<p>Now we will download Hadoop!</p>
<blockquote>
  <p>In the interest of time and simplicity, we will be installing the pre-compiled Hadoop binary rather than compiling Hadoop from the source code on the Orange Pi. However, compiling Hadoop so that it runs natively would be best for a production system. See <a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#InstallRaspbian_and_prepare_environment_for_Hadoop">Jonas Widriksson’s Tutorial</a> for instructions for compiling Hadoop from source on a Raspberry Pi. The Raspberry Pi OS, Raspbian Linux, is derived from Debian Linux so the instructions should match very well.</p>
</blockquote>

<p>Go to http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz and <code class="highlighter-rouge">wget</code> the Hadoop binary from the suggested mirror for the download.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>wget http://www.namesdir.com/mirrors/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz

<span class="gp">$ </span>sudo tar xzvf hadoop-2.7.3.tar.gz -C /opt
</code></pre>
</div>

<p>Let’s change the ownership of the Hadoop files to the <code class="highlighter-rouge">hduser</code> account. This will allow the Hadoop processes to make chages to these files without being <code class="highlighter-rouge">root</code> or having <code class="highlighter-rouge">sudo</code> permissions.</p>

<p>Change to the <code class="highlighter-rouge">/opt</code> folder where the Hadoop files are located and change the ownership recursively (changes all files in the folder) to the hduser account and the hadoop group.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> /opt
<span class="gp">$ </span>sudo chown -R hduser:hadoop hadoop-2.7.3
</code></pre>
</div>

<h4 id="configure-hadoop-environment">Configure Hadoop environment</h4>
<p>Now we will configure the hduser account to run Hadoop services.</p>

<blockquote>
  <p><em>In-depth info below. Skip for the next step in the journal.</em></p>

  <p>If you try to run Hadoop now, you’ll get an error:</p>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hadoop
-su: hadoop: <span class="nb">command </span>not found
</code></pre>
  </div>
  <p>That’s because of how Debian runs commands from the CLI. When you type a command, it looks in the <code class="highlighter-rouge">$PATH</code> environment variable for a link to an executable file. The <code class="highlighter-rouge">$PATH</code> variable is a list of directories where executable files are located on the filesystem, separated by a <code class="highlighter-rouge">:</code>. For example:</p>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">echo</span> <span class="nv">$PATH</span>
/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:
</code></pre>
  </div>
  <p>That’s how Debian (and Unix-like OSes) can magically execute programs without you typing the whole path to the executable.</p>
</blockquote>

<p>Change to the <code class="highlighter-rouge">hduser</code> account.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>su hduser
</code></pre>
</div>
<p>Add Hadoop references to the <code class="highlighter-rouge">hduser</code> profile.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>nano ~/.bashrc <span class="c">#Edit the hduser profile</span>
</code></pre>
</div>
<p>Append the following lines to the end of the file to add the directories <code class="highlighter-rouge">$PATH</code>.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/opt/jdk1.8.0_121
<span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/opt/hadoop-2.7.3
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
<span class="nb">export </span><span class="nv">YARN_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_HOME</span>/bin:<span class="nv">$HADOOP_HOME</span>/sbin:<span class="nv">$JAVA_HOME</span>/bin
</code></pre>
</div>
<p>Press <code class="highlighter-rouge">Ctrl-o</code> to write-out (it’s like save) and <code class="highlighter-rouge">Crtl-x</code> to exit <code class="highlighter-rouge">nano</code> after writing-out.</p>

<p>Apply the new <code class="highlighter-rouge">.bashrc</code>.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">source</span> ~/.bashrc
</code></pre>
</div>
<p>We need to add <code class="highlighter-rouge">$JAVA_HOME</code> to the <code class="highlighter-rouge">hadoop-env.sh</code> file too.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">cd</span> <span class="nv">$HADOOP_CONF_DIR</span>
nano hadoop-env.sh
</code></pre>
</div>
<p>Add the <code class="highlighter-rouge">JAVA_HOME</code> link from above. Change</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># The java implementation to use.</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">JAVA_HOME</span><span class="k">}</span>
</code></pre>
</div>
<p>to</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># The java implementation to use.</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/opt/jdk1.8.0_121/
</code></pre>
</div>
<p>It seems that hard coding the link is helpful for a distributed cluster, as we are making. This excerpt is from the <code class="highlighter-rouge">hadoop-env.sh</code> file:</p>
<blockquote>
  <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># Set Hadoop-specific environment variables here.</span>

<span class="c"># The only required environment variable is JAVA_HOME.  All others are</span>
<span class="c"># optional.  When running a distributed configuration it is best to</span>
<span class="c"># set JAVA_HOME in this file, so that it is correctly defined on</span>
<span class="c"># remote nodes.</span>

<span class="c"># The java implementation to use.</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">JAVA_HOME</span><span class="k">}</span>
</code></pre>
  </div>
</blockquote>

<p>Ensure the environment variables reference Hadoop properly</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># "hadoop" is in the $PATH now,</span>
<span class="c"># so we can execute it from the CLI!</span>

<span class="gp">$ </span>hadoop version

Hadoop 2.7.3
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff
Compiled by root on 2016-08-18T01:41Z
Compiled with protoc 2.5.0
From <span class="nb">source </span>with checksum 2e4ce5f957ea4db193bce3734ff29ff4
This <span class="nb">command </span>was run using /opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar
</code></pre>
</div>
<p>If you get an error, one of the environment variables is not properly set.</p>

<p>At this point Hadoop is installed. Yay! However, the services and configuration of Hadoop itself are not setup. So onto configuration!</p>

<h4 id="installing-hadoop-273-references">Installing Hadoop 2.7.3 References</h4>
<ol>
  <li><a href="http://hadoop.apache.org/releases.html">Apache Hadoop Releases</a></li>
  <li><a href="http://www.piprojects.xyz/install-hadoop-java-orange-pi/">Install Hadoop on the Orange Pi</a></li>
  <li><a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/">Jonas Widriksson’s Tutorial</a></li>
  <li><a href="http://www.widriksson.com/raspberry-pi-hadoop-cluster/">Jonas Widriksson’s Original Hadoop v1 Tutorial</a></li>
</ol>

<hr />

<h3 id="connect-hadoop-cluster">Connect Hadoop Cluster</h3>
<p><em>This section of the journal draws from two tutorials:</em></p>
<ul>
  <li><em><a href="http://www.piprojects.xyz/install-hadoop-java-orange-pi/">Install Hadoop on the Orange Pi</a> tutorial from <a href="http://www.piprojects.xyz/">Pi Projects</a></em></li>
  <li><em>Multi-node cluster configuration: <a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#InstallRaspbian_and_prepare_environment_for_Hadoop">Jonas Widriksson’s Tutorial</a> for setting up a Hadoop 2.7.2 cluster on Raspbery Pis.</em></li>
</ul>

<h4 id="network-access">Network Access</h4>
<p>Hadoop’s services span multiple computers (called nodes) connected by a network to provide the big-data crunching ability it is known for. We have to set up each node to recognize the other nodes on the network.</p>

<h5 id="hostname">Hostname</h5>
<p>First, let’s name each node. Debian uses the <code class="highlighter-rouge">/etc/hostname</code> file to establish any given machine’s name on the network, called <a href="https://debian-handbook.info/browse/stable/sect.hostname-name-service.html">hostname</a>.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo nano /etc/hostname
</code></pre>
</div>
<p>Delete the name that’s there, and give it a new one. For each node, I’m using <code class="highlighter-rouge">hadoopnode</code> and an incrementing integer.
For example:
<img src="/images/nano-hostname-1.png" alt="nano /etc/hostname" /></p>

<h5 id="static-ip-addresses">Static IP Addresses</h5>
<p>Next, give each node a static IP address on your router or network switch. For example:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>192.168.0.110    hadoopnode1
192.168.0.111    hadoopnode2
...
etc.
</code></pre>
</div>
<p>This means that every time the single-board computer requests and IP address with DHCP (read more <a href="https://kb.iu.edu/d/adov">here</a>), the router gives it the same one. This makes it simple for the single-board computers to access one another over the network, and we don’t have to set any network settings on each node.</p>

<p><strong>To find the instructions</strong>, it will be best just to Google search your router or switch and “static IP address”, since each router/switch interface is different.</p>

<h5 id="hosts">Hosts</h5>
<p>We also need to tell each node what the names of the other nodes are. <a href="https://debian-handbook.info/browse/stable/sect.hostname-name-service.html">Debian</a> uses the <code class="highlighter-rouge">/etc/hosts</code> file as a hostname table, which is a type of simple local DNS service.
Create the hosts file with <strong>every</strong> hostname and static IP address:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>192.168.0.110 hadoopnode1
192.168.0.111 hadoopnode2
</code></pre>
</div>

<h4 id="ssh-access">SSH Access</h4>

<p>Now we have names and IP addresses, each node will need to access data on the other nodes. We will allow this by creating a single SSH key pair to be shared among all nodes. Read more about SSH key pairs <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2">here</a>.</p>

<p>Create the SSH key pair with blank password:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>mkdir ~/.ssh
<span class="gp">$ </span>ssh-keygen -t rsa -P <span class="s2">""</span> <span class="c"># -t rsa specifies key type</span>
                          <span class="c"># -P "" specifies blank password</span>
<span class="gp">$ </span>cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys <span class="c"># Copies public (.pub) key into the publicly</span>
                                                 <span class="c"># accessible directory for authorized login keys</span>
</code></pre>
</div>
<p>Now login to this machine to add the machine’s certificate to the <code class="highlighter-rouge">~/.ssh/known_hosts</code> directory:</p>

<p>(Answer ‘yes’ when asked to trust the host certificate. This allows Hadoop to login among the cluster later.)</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ssh localhost
</code></pre>
</div>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">logout</span> <span class="c"># closes the ssh session and returns</span>
         <span class="c"># you to your former session</span>
</code></pre>
</div>
<p>Now your <code class="highlighter-rouge">~/.ssh</code> folder should look like this:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ls ~/.ssh
authorized_keys  id_rsa  id_rsa.pub  known_hosts
</code></pre>
</div>

<h4 id="networking-references">Networking References</h4>
<ol>
  <li><a href="https://debian-handbook.info/browse/stable/sect.hostname-name-service.html">Hostname - Debian Manual</a></li>
  <li><a href="https://kb.iu.edu/d/adov">DHCP</a></li>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2">SSH Keys</a></li>
</ol>

<hr />

<h3 id="hadoop-configuration">Hadoop Configuration</h3>
<h4 id="cluster-settings">Cluster Settings</h4>
<p>Now we will setup the Hadoop system itself. There are 5 relevant setting files that we need to configure. We will leave the rest as default.</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">$HADOOP_CONF_DIR/slaves</code>:</p>

    <p>Replace the contents with the names of the nodes in your cluster:</p>
    <div class="language-bash highlighter-rouge"><pre class="highlight"><code> <span class="nv">$ </span>nano <span class="nv">$HADOOP_CONF_DIR</span>/slaves
</code></pre>
    </div>
    <p><em>Example</em></p>
    <div class="highlighter-rouge"><pre class="highlight"><code> hadoopnode1
 hadoopnode2
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">$HADOOP_CONF_DIR/core-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>/hdfs/tmp<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>fs.default.name<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hdfs://hadoopnode1:54310<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">$HADOOP_CONF_DIR/hdfs-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">$HADOOP_CONF_DIR/mapred-site.xml</code>:</p>

    <p>Don’t panic if you can’t find this one. It doesn’t exist by default. We need to create it from the template provided.</p>

    <div class="language-bash highlighter-rouge"><pre class="highlight"><code> <span class="c"># Creates the .xml file from the template by copying</span>
 <span class="nv">$ </span>cp <span class="nv">$HADOOP_CONF_DIR</span>/mapred-site.xml.template <span class="nv">$HADOOP_CONF_DIR</span>/mapred-site.xml
 <span class="nv">$ </span>nano mapred-site.xml
</code></pre>
    </div>
    <p>Add the following property:</p>
    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">$HADOOP_CONF_DIR/yarn-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hadoopnode1:8025<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hadoopnode1:8035<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hadoopnode1:8050<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
</ol>

<p>YARN is the resource negotiator for Hadoop. It allocates the memory and CPU resources for each Hadoop job. In addition to the basic settings below, we can also configure how YARN distributes the system resources in this file. Understanding and configuring all these settings is out of the scope of this journal, however. See the <a href="https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop 2.7.3 YARN Docs</a> for the complete details.</p>

<h4 id="create-hdfs">Create HDFS</h4>
<p>The Hadoop distributed filesystem (hdfs) is one of the foundations of the Hadoop framework. It allows files to be saves across nodes in the cluster, and replicates data to safeguard against data loss. It is separate from the filesystem of each node, and files can be placed into the hdfs from the normal filesystem using commmands to the namenode.
Create hdfs:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo mkdir -p /hdfs/tmp
<span class="gp">$ </span>sudo chown hduser:hadoop /hdfs/tmp
<span class="gp">$ </span>chmod 750 /hdfs/tmp <span class="c"># XxX</span>
</code></pre>
</div>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs namenode -format <span class="c"># XxX Explanation?</span>
</code></pre>
</div>

<h4 id="hadoop-configuration-references">Hadoop Configuration References</h4>
<ol>
  <li><a href="https://hadoop.apache.org/docs/r2.7.3/">Apache Hadoop 2.7.3 Docs</a></li>
  <li><a href="https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop 2.7.3 YARN Docs</a></li>
  <li><a href="http://www.piprojects.xyz/install-hadoop-java-orange-pi/">Install Hadoop on the Orange Pi</a></li>
  <li><a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/">Jonas Widriksson’s Tutorial</a></li>
</ol>

<hr />

<h3 id="create-hadoop-nodes">Create Hadoop Nodes</h3>
<h4 id="clone-sd-card">Clone SD Card</h4>
<p>At this point all the basic configuration is made. We only lack starting up the Hadoop cluster!</p>

<blockquote>
  <p>Up until this point, we have operated only on 1 physical single-board computer and 1 physical SD card. That allows us to have a single “source of truth” for all the installation and configuration. Now we will just clone the OS onto as many more SD cards as we have nodes in the cluster. We don’t even have to install the OS on the other nodes! We will just copy the one we’ve already made.</p>
</blockquote>

<p>Let’s clone the SD card. I am using macOS, so I will give the Unix-like CLI instructions. Windows users can use <a href="http://lifehacker.com/how-to-clone-your-raspberry-pi-sd-card-for-super-easy-r-1261113524">Win32DiskImager</a>. For a description of the process on multiple platforms, <a href="https://raspberrypi.stackexchange.com/questions/311/how-do-i-backup-my-raspberry-pi">see here</a>.</p>

<p>Regardless of the tool you use to clone the OS image, we will use <a href="https://etcher.io">Etcher</a> to flash each new microSD card for each new node in the cluster, just like we did for the first flash (see <a href="#flash-the-os">Flash the OS</a> above).</p>

<h5 id="clone-the-os-to-a-file">Clone the OS to a File</h5>
<p>First poweroff the node:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo poweroff
</code></pre>
</div>
<p>Eject the microSD card, and insert it into another computer (using the microSD adpater).</p>

<p>Open a CLI (Terminal on macOS) and run the following commands:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>diskutil list
</code></pre>
</div>
<p>Here is my output:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>/dev/disk0 (internal, physical):
   #:                   TYPE NAME            SIZE      IDENTIFIER
   0:  GUID_partition_scheme                *250.1 GB  disk0
   1:                    EFI EFI             209.7 MB  disk0s1
   2:      Apple_CoreStorage Macintosh SSD   249.2 GB  disk0s2
   3:             Apple_Boot Recovery HD     650.0 MB  disk0s3

/dev/disk1 (internal, virtual):
   #:                   TYPE NAME            SIZE      IDENTIFIER
   0:                        Macintosh SSD  +248.8 GB  disk1

/dev/disk3 (internal, physical): (&lt;&lt;&lt; Armbian OS SD Card Here)
   #:                   TYPE NAME       SIZE     IDENTIFIER
   0: FDisk_partition_scheme           *15.9 GB  disk3
   1:                  Linux            15.6 GB  disk3s1
</code></pre>
</div>
<p>Let’s unmount the SD card file system:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>diskutil unmountDisk /dev/disk3
Unmount of all volumes on disk3 was successful
</code></pre>
</div>
<blockquote>
  <p>Be sure to do this. I tried to clone the OS image without unmounting first and got an error. Oops!</p>
</blockquote>

<p>We’ll use the <code class="highlighter-rouge">dd</code> command to clone the OS image:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>$ sudo dd if=/dev/rdisk3 of=/path/to/new/imagefile.img bs=1m
</code></pre>
</div>
<p>Explanation:</p>
<ul>
  <li><code class="highlighter-rouge">rdisk3</code>: This is the same as <code class="highlighter-rouge">disk3</code> above. <code class="highlighter-rouge">rdisk</code> uses a <a href="https://superuser.com/questions/631592/why-is-dev-rdisk-about-20-times-faster-than-dev-disk-in-mac-os-x">different, much faster access protocol</a> than <code class="highlighter-rouge">disk</code>, however.</li>
  <li><code class="highlighter-rouge">if=</code>:<code class="highlighter-rouge">/file/to/SD_Card</code></li>
  <li><code class="highlighter-rouge">of=</code>:<code class="highlighter-rouge">/file/to/.img/file</code></li>
  <li><code class="highlighter-rouge">bs=1m</code>: Size of the transfer block</li>
</ul>

<p>It took about 6 min for me. Output:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>15193+1 records in
15193+1 records out
15931539456 bytes transferred in 355.982354 secs (44753734 bytes/sec)
</code></pre>
</div>

<h5 id="flash-the-os-image">Flash the OS Image</h5>
<p>Now let’s flash the OS image to every microSD for the cluster. We need one for each node. The process is the same one we used for the original Armbian OS image flash, without the extraction (<code class="highlighter-rouge">dd</code> outputs an uncompressed file the same size as the OS filesystem).</p>

<p>Use <a href="https://etcher.io">Etcher</a> from <a href="https://resin.io/?ref=etcher">Resin.io</a>. See section <a href="#flash-the-os">Flash the OS</a> above.</p>

<p>Once all the microSD cards have been flashed with the OS image which already has Hadoop installed and configured, let’s boot them all up and set the final, individual settings!</p>

<h4 id="set-node-hostnames">Set Node Hostnames</h4>
<p>Just like we set the hostname of the original node as <code class="highlighter-rouge">hadoopnode1</code>, we will need to set the <code class="highlighter-rouge">hostname</code> of each node in the cluster. I am using incrementing integers:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo nano /etc/hostname
</code></pre>
</div>

<p><img src="/images/nano-hostname-2.png" alt="nano /etc/hostname" /></p>

<h4 id="cluster-setup-references">Cluster Setup References</h4>
<ol>
  <li><a href="https://raspberrypi.stackexchange.com/questions/311/how-do-i-backup-my-raspberry-pi">Clone Raspberry Pi SD Card</a></li>
  <li><a href="https://superuser.com/questions/631592/why-is-dev-rdisk-about-20-times-faster-than-dev-disk-in-mac-os-x">rdisk Access Protocol</a></li>
</ol>

<hr />

<h3 id="start-hadoop">Start Hadoop</h3>
<p>Now that we have configured Hadoop, cloned the OS (with Hadoop configuration) to each node, and set the hostnames, let’s start this Hadoop cluster up!</p>

<p>Start the hdfs and YARN:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-dfs
<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-yarn.sh
</code></pre>
</div>
<p>If you haven’t logged into one or more of the nodes as we did above in <a href="#ssh-access">SSH Acess</a>, it will ask whether to trust the Host Key.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>The authenticity of host 'namenode2 (192.168.0.110)' can't be established.
ECDSA key fingerprint is xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx.
Are you sure you want to continue connecting (yes/no)?
</code></pre>
</div>
<p>Enter yes.</p>

<p>It may ask you to trust the authenticity of host <code class="highlighter-rouge">(0.0.0.0)</code>, which is the machine itself. Enter yes for this too.</p>

<p>Now, verify that all services are running:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>jps
</code></pre>
</div>
<p>If everything started correctly, you’ll see these services on the <strong>NameNode</strong> (hadoopnode1 for us):</p>
<div class="highlighter-rouge"><pre class="highlight"><code>3218 NameNode
3620 SecondaryNameNode
3992 NodeManager
3817 ResourceManager
3387 DataNode
4429 Jps
</code></pre>
</div>
<p><em>If you don’t get a <code class="highlighter-rouge">SecondaryNameNode</code>, make sure you trusted <code class="highlighter-rouge">0.0.0.0</code> during the connection. <code class="highlighter-rouge">SecondaryNameNode</code> runs on the localhost (<code class="highlighter-rouge">0.0.0.0</code>) in this configuration.</em></p>

<p>And these on the <strong>Slave Nodes</strong> (hadoopnode2 for us):</p>
<div class="highlighter-rouge"><pre class="highlight"><code>3108 DataNode
3434 Jps
3290 NodeManager
</code></pre>
</div>

<p>Additionally, a Hadoop cluster health report can be obtained on the CLI with:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs -dfsadmin -reports
</code></pre>
</div>
<p>The <code class="highlighter-rouge">hdfs dfsadmin</code> command has lots of other administrative options besides <code class="highlighter-rouge">-report</code>.</p>

<p>Let’s take a moment to celebrate before we test out the hadoop cluster with the <a href="http://www.informit.com/articles/article.aspx?p=2190194&amp;seqNum=3">Hadoop pi test program</a> from informIT®!</p>

<hr />

<h3 id="test-hadoop">Test Hadoop</h3>
<p>Hadoop ships with a .jar of example MapReduce applications, located at <code class="highlighter-rouge">$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar</code></p>

<p>You can run these with this command:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> <span class="nv">$HADOOP_HOME</span>/share/hadoop/mapreduce
<span class="gp">$ </span>yarn jar hadoop-mapreduce-examples-2.7.3.jar &lt;APPLICATION_NAME&gt; &lt;OPTIONS&gt;

<span class="c">#Example</span>
<span class="gp">$ </span>yarn hadoop-mapreduce-examples-2.7.3.jar pi 16 100
</code></pre>
</div>
<p>To make this faster in the future, I exported the path to an alias:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">export </span><span class="nv">MAPRED_EXAMPLES</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/share/hadoop/mapreduce

<span class="c"># You can append this to ~/.bashrc to apply it on any shell</span>
</code></pre>
</div>
<p>Run the .jar with no parameters to see all the applications available.</p>

<h4 id="error">Error!</h4>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>yarn jar hadoop-mapreduce-examples-2.7.3.jar pi 16 100
</code></pre>
</div>
<p>The job fails. Hadoop reports that it <strong>cannot find a certain block in the HDFS</strong>.</p>

<p>Suspecting that some of the blocks became corrupted after a previous power interruption, I reformatted the HDFS, but to no avail:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>rm -rf /hdfs/tmp/<span class="k">*</span>
hdfs namenode -format
</code></pre>
</div>
<p>I also reconfigured the YARN, MapReduce, core-site, and HDFS settings according to <a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#YARN_and_MapReduce_memory_configuration_overview">Jonas Widriksson’s Blog</a>, only I divided all the memory values by 2, because his machines (Raspberry Pi 3s) have 2x the memory that mine do.</p>

<h4 id="new-error">New Error</h4>
<p>At this point I started getting <strong>Out-of-memory</strong> errors, something like <strong>“Container exceeded the virtual memory limit, and the container was killed.”</strong></p>

<p>I’ll shutdown the cluster and try again tomorrow.</p>

<h4 id="more-errors">More Errors</h4>
<p>Now I get network-related errors. Something like <strong>“Connection to the ResourceManager timed out.”</strong></p>

<p>At this point, I think I will just start over from scratch.</p>

<h2 id="troubleshooting-hadoop">Troubleshooting Hadoop</h2>
<p>At this point, I have no idea what is malfunctioning with this cluster. The reported cause of a MapReduce job failing seems to be different every time!</p>

<p>I’ll try to collect all the logs I can for analysis, gather some more tutorials, and troubleshoot the namenode. Then I’ll clone the OS and restart the cluster.</p>

<blockquote>
  <p>NOTE: The Hadoop logging protocol is very complicated, and after several hours of attempting to find the cause of the job failure, I decided that it is not worth spending even more time to discover the root cause of the failure. I’ll just troubleshoot “from the ground up”</p>
</blockquote>

<p>Here’s what I tried:</p>

<h3 id="ensure-hostnames-working-properly">Ensure Hostnames Working Properly</h3>
<p>Include the Hadoop cluster ndoes in my client machine’s (MacBook Pro) <code class="highlighter-rouge">etc/hosts</code> file:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo nano /etc/hosts
</code></pre>
</div>
<p>Add the Hadoop nodes:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c">### Hadoop Cluster Nodes - Orange Pi ###</span>
192.168.0.110	hadoopnode1
192.168.0.111	hadoopnode2
</code></pre>
</div>
<p>This now works from my client machine:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ssh hduser@hadoopnode1
</code></pre>
</div>

<hr />

<h3 id="check-the-java-version">Check the Java version</h3>
<p>Check to see if the java version is correct:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>java -version

java version <span class="s2">"1.8.0_121"</span>
Java<span class="o">(</span>TM<span class="o">)</span> SE Runtime Environment <span class="o">(</span>build 1.8.0_121-b13<span class="o">)</span>
Java HotSpot<span class="o">(</span>TM<span class="o">)</span> Client VM <span class="o">(</span>build 25.121-b13, mixed mode<span class="o">)</span>
</code></pre>
</div>
<p>Java 1.8.0 seems to be the version I installed, so that looks right.</p>

<hr />

<h3 id="check-hostnames">Check Hostnames</h3>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>cat /etc/hosts

127.0.0.1   localhost orangepione
::1         localhost orangepione ip6-localhost ip6-loopback
fe00::0     ip6-localnet
ff00::0     ip6-mcastprefix
ff02::1     ip6-allnodes
ff02::2     ip6-allrouters
192.168.0.110 hadoopnode1
192.168.0.111 hadoopnode2
</code></pre>
</div>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>cat /etc/hostname

hadoopnode1
</code></pre>
</div>
<p>There was an extra newline after <code class="highlighter-rouge">hadoopnode1</code>, so I deleted that.</p>

<hr />

<h3 id="ssh-access-1">SSH Access</h3>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>cat ~/.ssh/authorized_keys

ssh-rsa &lt;KEY REDACTED FOR SECURITY&gt; hduser@orangepione
</code></pre>
</div>
<p>Aha! We might be on to something here. The SSH key is for hduser@orangepione, which doesn’t exist anymore. We changed the hostname from orangepione to hadoopnode1/2.</p>

<p>Let’s delete the <code class="highlighter-rouge">~/.ssh</code> directory and recreate the SSH key:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ssh-keygen -t rsa -P <span class="s2">""</span>
<span class="gp">$ </span>cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys
<span class="c"># Replace    ^^^^^^^^^^ with the directory you choose</span>
<span class="c"># in the ssh-keygen command. id_rsa is default.</span>
</code></pre>
</div>
<p>Copying the <code class="highlighter-rouge">id_rsa.pub</code> (public key) into the <code class="highlighter-rouge">authorized_keys</code> file allows any machine with the <code class="highlighter-rouge">id_rsa</code> (private key) to login without a password.</p>

<p><a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-on-a-vps">Rsync</a> is a great way to clone the folder to the home folder of the other nodes:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>rsync -av ~/.ssh hduser@hadoopnode2:~ <span class="c"># sync the .ssh folder to the ~ directory of the other node.</span>
</code></pre>
</div>

<p>Ensure the SSH key allows passwordless login:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ssh hduser@hadoopnode1 <span class="c"># It works! (or not)</span>
<span class="gp">$ </span><span class="nb">exit</span>
</code></pre>
</div>

<p><em>Update: Deleting the hostname after the public key in <code class="highlighter-rouge">id_rsa.pub</code> and in <code class="highlighter-rouge">authorized_keys</code> did not change the ability to SSH passwordless, so that is probably not the issue.</em></p>

<hr />

<h3 id="hadoop-install-verification">Hadoop Install Verification</h3>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hadoop version

Hadoop 2.7.3
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff
Compiled by root on 2016-08-18T01:41Z
Compiled with protoc 2.5.0
From <span class="nb">source </span>with checksum 2e4ce5f957ea4db193bce3734ff29ff4
This <span class="nb">command </span>was run using /opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar
</code></pre>
</div>
<p>Seems to be Hadoop 2.7.3, so that’s right.</p>

<hr />

<h3 id="hdfs-fsck">HDFS <code class="highlighter-rouge">fsck</code></h3>

<p>I ran the Hadoop filesystem check:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs fsck / <span class="c"># Check the whole dfs, starting from /</span>

..................Status: HEALTHY
 Total size:	1508057 B
 Total <span class="nb">dirs</span>:	11
 Total files:	21
 Total symlinks:		0
 Total blocks <span class="o">(</span>validated<span class="o">)</span>:	21 <span class="o">(</span>avg. block size 71812 B<span class="o">)</span>
 Minimally replicated blocks:	21 <span class="o">(</span>100.0 %<span class="o">)</span>
 Over-replicated blocks:	0 <span class="o">(</span>0.0 %<span class="o">)</span>
 Under-replicated blocks:	2 <span class="o">(</span>9.523809 %<span class="o">)</span>
 Mis-replicated blocks:		0 <span class="o">(</span>0.0 %<span class="o">)</span>
 Default replication factor:	2
 Average block replication:	2.0
 Corrupt blocks:		0
 Missing replicas:		16 <span class="o">(</span>27.586206 %<span class="o">)</span>
 Number of data-nodes:		2
 Number of racks:		1
FSCK ended at Sat Apr 22 11:44:53 CDT 2017 <span class="k">in </span>126 milliseconds


The filesystem under path <span class="s1">'/'</span> is HEALTHY
</code></pre>
</div>
<p>It looks like corrupted blocks is not the issue. I’m guessing the 2 <code class="highlighter-rouge">under-replicated blocks</code> are due to failed jobs.</p>

<hr />

<h3 id="reformat-hdfs">Reformat HDFS</h3>
<p>I saved all the HDFS files to my client machine:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfs -get / ~/HdfsDump
<span class="gp">$ </span>scp -r HdfsDump user@clientipaddress:~

<span class="gp">$ </span>scp -r /hdfs user@clientipaddress:~
</code></pre>
</div>
<p>Delete all HDFS files and reformat:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>rm -rf /hdfs/<span class="k">*</span>
<span class="gp">$ </span>hdfs namenode -format
</code></pre>
</div>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfsadmin -report
</code></pre>
</div>
<p>After starting the <code class="highlighter-rouge">dfs</code> and <code class="highlighter-rouge">yarn</code>, only <code class="highlighter-rouge">hadoopnode1</code> is connected. This may be due to a cluster ID conflict.</p>

<p>I’ll remove all the files on both nodes and try again, using the hdfs directory structure from <a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-3-7d114d35fdf1">Jason Carter’s blog</a> (parts 2 &amp; 3 specifically):</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo rm -rf /hdfs <span class="c"># hadoopnode1 &amp; 2</span>
<span class="gp">$ </span>sudo mkdir -p /hdfs/namenode <span class="c"># only hadoopnode1</span>
<span class="gp">$ </span>sudo mkdir -p /hdfs/datanode <span class="c"># hadoopnode1 &amp; 2</span>
<span class="gp">$ </span>sudo chown hduser:hadoop /hdfs/ -R <span class="c"># change directory ownership</span>
<span class="gp">$ </span>chmod 750 /hdfs <span class="c"># change directory permissions</span>
<span class="gp">$ </span>ls -l / | grep hdfs <span class="c"># view directory</span>

drwxr-x---  4 hduser hadoop  4096 Apr 22 12:57 hdfs
</code></pre>
</div>
<p>Change configuration to reflect new <code class="highlighter-rouge">/hdfs</code> directory structure:</p>
<div class="language-xml highlighter-rouge"><pre class="highlight"><code>File: hdfs-site.xml

<span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.blocksize<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>5242880<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>file:/hdfs/namenode<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.datanode.name.dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>file:/hdfs/datanode<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre>
</div>
<p>Sync to <code class="highlighter-rouge">hadoopnode2</code>:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>rsync -av <span class="nv">$HADOOP_CONF_DIR</span>/ hduser@hadoopnode2:<span class="nv">$HADOOP_CONF_DIR</span>
<span class="c"># rsync -anv will do a dry-run so you can see the specific files</span>
<span class="c"># to be sent before you actually send anything.</span>
</code></pre>
</div>
<p>Create HDFS again:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># On the namenode (hadoopnode1 for me)</span>
<span class="gp">$ </span>hdfs namenode -format

<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-dfs.sh
<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-yarn.sh
</code></pre>
</div>
<p>Check the status:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfsadmin -report

Configured Capacity: 30753898496 <span class="o">(</span>28.64 GB<span class="o">)</span>
Present Capacity: 26473508864 <span class="o">(</span>24.66 GB<span class="o">)</span>
DFS Remaining: 26473459712 <span class="o">(</span>24.66 GB<span class="o">)</span>
DFS Used: 49152 <span class="o">(</span>48 KB<span class="o">)</span>
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks <span class="o">(</span>with replication factor 1<span class="o">)</span>: 0

-------------------------------------------------
Live datanodes <span class="o">(</span>2<span class="o">)</span>:

Name: 192.168.0.111:50010 <span class="o">(</span>hadoopnode2<span class="o">)</span>
Hostname: hadoopnode2
Decommission Status : Normal
Configured Capacity: 15376949248 <span class="o">(</span>14.32 GB<span class="o">)</span>
DFS Used: 24576 <span class="o">(</span>24 KB<span class="o">)</span>
Non DFS Used: 2121494528 <span class="o">(</span>1.98 GB<span class="o">)</span>
DFS Remaining: 13255430144 <span class="o">(</span>12.35 GB<span class="o">)</span>
DFS Used%: 0.00%
DFS Remaining%: 86.20%
Configured Cache Capacity: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Remaining: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 22 13:18:48 CDT 2017


Name: 192.168.0.110:50010 <span class="o">(</span>hadoopnode1<span class="o">)</span>
Hostname: hadoopnode1
Decommission Status : Normal
Configured Capacity: 15376949248 <span class="o">(</span>14.32 GB<span class="o">)</span>
DFS Used: 24576 <span class="o">(</span>24 KB<span class="o">)</span>
Non DFS Used: 2158895104 <span class="o">(</span>2.01 GB<span class="o">)</span>
DFS Remaining: 13218029568 <span class="o">(</span>12.31 GB<span class="o">)</span>
DFS Used%: 0.00%
DFS Remaining%: 85.96%
Configured Cache Capacity: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Remaining: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 22 13:18:50 CDT 2017
</code></pre>
</div>
<p>Both <code class="highlighter-rouge">datanodes</code> up!</p>

<hr />

<h3 id="hadoop-configuration-1">Hadoop Configuration</h3>

<h4 id="change-configuration-scheme">Change Configuration Scheme</h4>
<p>I’m going to try the Hadoop configuration from <a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-3-7d114d35fdf1">Jason Carter’s blog</a> (parts 2 &amp; 3 specifically), as we did above.</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">yarn-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
 <span class="c">&lt;!-- Site specific YARN configuration properties --&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hadoopnode1<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">hdfs-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.blocksize<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>5242880<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>file:/hdfs/namenode<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.datanode.name.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>file:/hdfs/datanode<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">mapred-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">core-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hdfs://hadoopnode1:53410<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>

    <blockquote>
      <p>I noticed here that I left the following setting in <code class="highlighter-rouge">core-site.xml</code> when I reformatted the HDFS:</p>
      <div class="language-xml highlighter-rouge"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>/hdfs/tmp/<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre>
      </div>
      <p>I should probably reformat the HDFS again to make it comply with the new structure.</p>
    </blockquote>
  </li>
  <li>
    <p><code class="highlighter-rouge">hadoop-env.sh</code>
Uncomment the following <code class="highlighter-rouge">export</code> line to define the HADOOP_HEAPSIZE parameter.</p>
    <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># The maximum amount of heap to use, in MB. Default is 1000.</span>
<span class="nb">export </span><span class="nv">HADOOP_HEAPSIZE</span><span class="o">=</span>128
</code></pre>
    </div>
    <p>Sync to <code class="highlighter-rouge">hadoopnode2</code>:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>rsync -av <span class="nv">$HADOOP_CONF_DIR</span>/ hduser@hadoopnode2:<span class="nv">$HADOOP_CONF_DIR</span>
<span class="c"># hdfs-site.xml won't be sent because we didn't make any new changes since the HDFS reformat ones.</span>

sending incremental file list
core-site.xml
hadoop-env.sh
mapred-site.xml
yarn-site.xml

sent 2,369 bytes  received 182 bytes  1,020.40 bytes/sec
total size is 78,384  speedup is 30.73
</code></pre>
</div>
<h4 id="reformat-hdfs-1">Reformat HDFS:</h4>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/stop-dfs.sh
<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/stop-yarn.sh
</code></pre>
</div>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo rm -rf /hdfs
<span class="gp">$ </span>sudo mkdir -p /hdfs/namenode <span class="c"># only hadoopnode1</span>
<span class="gp">$ </span>sudo mkdir -p /hdfs/datanode <span class="c"># hadoopnode1 &amp; 2</span>
<span class="gp">$ </span>sudo chown hduser:hadoop /hdfs/ -R <span class="c"># change directory ownership</span>
<span class="gp">$ </span>chmod 750 /hdfs <span class="c"># change directory permissions</span>
<span class="gp">$ </span>ls -l / | grep hdfs <span class="c"># view directory</span>

drwxr-x---  4 hduser hadoop  4096 Apr 22 14:08 hdfs
</code></pre>
</div>
<p>Create HDFS again (again):</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># On the namenode (hadoopnode1 for me)</span>
<span class="gp">$ </span>hdfs namenode -format

<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-dfs.sh
<span class="gp">$ </span><span class="nv">$HADOOP_HOME</span>/sbin/start-yarn.sh
</code></pre>
</div>
<p>Check the status:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfsadmin -report
</code></pre>
</div>
<p>Both <code class="highlighter-rouge">datanodes</code> up, and no <code class="highlighter-rouge">/tmp</code> directory:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># On namenode</span>
<span class="gp">$ </span>ls /hdfs
datanode  namenode
</code></pre>
</div>

<hr />

<h3 id="word-count-test">Word Count Test</h3>
<p>Load a <code class="highlighter-rouge">.txt</code> file from the client machine and put it into HDFS:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfs -put /path/to/local/file.txt /path/to/HDFS/file.txt
</code></pre>
</div>
<p>Run a wordcount job from the Hadoop example MapReduce applications. We are using a 1.1 MB plain text book from <a href="http://www.gutenberg.org/">Project Gutenberg</a>:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> /opt/hadoop-2.7.3/share/hadoop/mapreduce
<span class="gp">$ </span>ls

hadoop-mapreduce-client-app-2.7.3.jar
hadoop-mapreduce-client-common-2.7.3.jar
hadoop-mapreduce-client-core-2.7.3.jar
hadoop-mapreduce-client-hs-2.7.3.jar
hadoop-mapreduce-client-hs-plugins-2.7.3.jar
hadoop-mapreduce-client-jobclient-2.7.3.jar
hadoop-mapreduce-client-jobclient-2.7.3-tests.jar
hadoop-mapreduce-client-shuffle-2.7.3.jar
hadoop-mapreduce-examples-2.7.3.jar
lib
lib-examples
sources

<span class="c"># Output directory in command must not exist! The job will</span>
<span class="c"># fail if the specified directory exists already.</span>
<span class="gp">$ </span>yarn jar hadoop-mapreduce-examples-2.7.3.jar wordcount /path/to/hdfs/file.txt /path/to/output/directory
</code></pre>
</div>
<h4 id="success">SUCCESS!</h4>
<p><strong>Whoop!</strong> CLI Output of first successful Job:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>Java HotSpot(TM) Client VM warning: You have loaded library /opt/hadoop-2.7.3/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c &lt;libfile&gt;', or link it with '-z noexecstack'.
17/04/22 14:25:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/22 14:25:50 INFO client.RMProxy: Connecting to ResourceManager at hadoopnode1/192.168.0.110:8032
17/04/22 14:25:56 INFO input.FileInputFormat: Total input paths to process : 1
17/04/22 14:25:56 INFO mapreduce.JobSubmitter: number of splits:1
17/04/22 14:25:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1492888277723_0001
17/04/22 14:25:59 INFO impl.YarnClientImpl: Submitted application application_1492888277723_0001
17/04/22 14:26:00 INFO mapreduce.Job: The url to track the job: http://hadoopnode1:8088/proxy/application_1492888277723_0001/
17/04/22 14:26:00 INFO mapreduce.Job: Running job: job_1492888277723_0001
17/04/22 14:26:39 INFO mapreduce.Job: Job job_1492888277723_0001 running in uber mode : false
17/04/22 14:26:39 INFO mapreduce.Job:  map 0% reduce 0%
17/04/22 14:27:10 INFO mapreduce.Job:  map 100% reduce 0%
17/04/22 14:27:35 INFO mapreduce.Job:  map 100% reduce 100%
17/04/22 14:27:36 INFO mapreduce.Job: Job job_1492888277723_0001 completed successfully
17/04/22 14:27:37 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=349637
		FILE: Number of bytes written=937211
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1111292
		HDFS: Number of bytes written=257189
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=27800
		Total time spent by all reduces in occupied slots (ms)=21129
		Total time spent by all map tasks (ms)=27800
		Total time spent by all reduce tasks (ms)=21129
		Total vcore-milliseconds taken by all map tasks=27800
		Total vcore-milliseconds taken by all reduce tasks=21129
		Total megabyte-milliseconds taken by all map tasks=28467200
		Total megabyte-milliseconds taken by all reduce tasks=21636096
	Map-Reduce Framework
      Map input records=19150
  		Map output records=184794
  		Map output bytes=1819171
  		Map output materialized bytes=349637
  		Input split bytes=99
  		Combine input records=184794
  		Combine output records=23648
  		Reduce input groups=23648
  		Reduce shuffle bytes=349637
  		Reduce input records=23648
  		Reduce output records=23648
  		Spilled Records=47296
  		Shuffled Maps =1
  		Failed Shuffles=0
  		Merged Map outputs=1
  		GC time elapsed (ms)=1003
  		CPU time spent (ms)=16010
  		Physical memory (bytes) snapshot=215564288
  		Virtual memory (bytes) snapshot=642580480
  		Total committed heap usage (bytes)=131350528
  	Shuffle Errors
  		BAD_ID=0
  		CONNECTION=0
  		IO_ERROR=0
  		WRONG_LENGTH=0
  		WRONG_MAP=0
  		WRONG_REDUCE=0
  	File Input Format Counters
  		Bytes Read=1111193
  	File Output Format Counters
  		Bytes Written=257189
</code></pre>
</div>
<p>Let’s look at the actual wordcount output. The output directory has two parts. (<a href="http://stackoverflow.com/questions/10666488/what-are-success-and-part-r-00000-files-in-hadoop/10666874#10666874">See here for a StackOverflow question about it</a>)</p>
<ol>
  <li><code class="highlighter-rouge">_SUCCESS</code> An empty file that signifies a successful job completion</li>
  <li>Raw output files
    <ul>
      <li><code class="highlighter-rouge">part-x-00000</code> Output of task number 00000</li>
      <li><code class="highlighter-rouge">part-x-00001</code> Output of task number 00001
 …</li>
      <li><code class="highlighter-rouge">part-x-yyyyy</code> Output of task number yyyyy</li>
    </ul>
  </li>
</ol>

<p><code class="highlighter-rouge">IliadOutput</code> contents:</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfs -ls IliadOutput

Found 2 items
<span class="c"># Signifies successful job</span>
-rw-r--r--   2 hduser supergroup          0 2017-04-22 14:27 /IliadCount/_SUCCESS

<span class="c"># Wordcount output file of the only reduce task</span>
-rw-r--r--   2 hduser supergroup     257189 2017-04-22 14:27 /IliadCount/part-r-00000
</code></pre>
</div>

<hr />

<h3 id="calculate-π-test">Calculate π Test</h3>
<p>With the previous configurations, we tried the pi MapReduce example application. Let’s try it again with the new configuration.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> /opt/hadoop-2.7.3/share/hadoop/mapreduce
<span class="gp">$ </span>ls

hadoop-mapreduce-client-app-2.7.3.jar
hadoop-mapreduce-client-common-2.7.3.jar
hadoop-mapreduce-client-core-2.7.3.jar
hadoop-mapreduce-client-hs-2.7.3.jar
hadoop-mapreduce-client-hs-plugins-2.7.3.jar
hadoop-mapreduce-client-jobclient-2.7.3.jar
hadoop-mapreduce-client-jobclient-2.7.3-tests.jar
hadoop-mapreduce-client-shuffle-2.7.3.jar
hadoop-mapreduce-examples-2.7.3.jar
lib
lib-examples
sources

<span class="gp">$ </span>yarn jar hadoop-mapreduce-examples-2.7.3.jar pi 16 1000
</code></pre>
</div>
<p>The job got stuck here for about 30 minutes.</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>INFO mapreduce.Job: Running job: job_1492888277723_0002
</code></pre>
</div>
<p>I think the resources of the Orange Pi are too small for the job  parameters. The touble is that I cannot find any description of the parameters for the calculate pi program anywhere.</p>

<p>I’ll try to reconfigure Hadoop to understand the resource limitations of the Orange Pi One.</p>

<hr />

<h3 id="optimize-configuration">Optimize Configuration</h3>
<p>Now I will try combining configurations:</p>
<ul>
  <li>General cluster settings from <a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-3-7d114d35fdf1">Jason Carter’s blog</a></li>
  <li>Specific memory-management settings used above in section <a href="#error">Error!</a> (from <a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#YARN_and_MapReduce_memory_configuration_overview">Jonas Widriksson’s Blog</a>)</li>
</ul>

<h4 id="configuration-3">Configuration #3</h4>

<ol>
  <li>
    <p><code class="highlighter-rouge">yarn-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hadoopnode1<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.memory-mb<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;description&gt;</span>Whether virtual memory limits will be enforced  for containers.<span class="nt">&lt;/description&gt;</span>
 <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">mapred-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>mapreduce.map.memory.mb<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>256<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>mapreduce.reduce.memory.mb<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
</ol>

<p>Propagate the changes with <code class="highlighter-rouge">rsync</code> as above.</p>
<blockquote>
  <p>The $HADOOP_HOME/sbin/start-dfs.sh script has started failing to start the DataNode service. Maybe reformatting the HDFS as above will correct the error.</p>
</blockquote>

<hr />

<h3 id="test-again">Test Again</h3>
<h4 id="calculate-pi">Calculate Pi</h4>

<p>The application failed with the following messages:</p>

<p><code class="highlighter-rouge">hadoopnode2</code></p>
<div class="highlighter-rouge"><pre class="highlight"><code>Diagnostics: Container [pid=3907,containerID=container_1492900593461_0001_01_000001] is running beyond virtual memory limits. Current usage: 16.7 MB of 128 MB physical memory used; 1.1 GB of 268.8 MB virtual memory used. Killing container.
</code></pre>
</div>
<p><code class="highlighter-rouge">hadoopnode1</code></p>
<div class="highlighter-rouge"><pre class="highlight"><code>ApplicationMaster for attempt appattempt_1492900593461_0001_000002 timed out
</code></pre>
</div>
<p>It seems that removing the virtual memory limit enforcement setting was not properly synced to <code class="highlighter-rouge">hadoopnode2</code>, and the container was killed for exceeding the allotment, which is (2.1 *  physical memory allotment). It also seems that the retry attempt on <code class="highlighter-rouge">hadoopnode1</code> timed out because the ApplicationMaster could not start a container. This seems to indicate insufficient memory settings.</p>

<h4 id="wordcount">Wordcount</h4>
<p>1.1 MB text file from <a href="http://www.gutenberg.org/">Project Gutenberg</a> again.</p>

<h5 id="test-job-1">Test Job 1:</h5>
<p><code class="highlighter-rouge">hadoopnode2</code></p>
<div class="highlighter-rouge"><pre class="highlight"><code>YarnException: Unauthorized request to start container
</code></pre>
</div>
<p><code class="highlighter-rouge">hadoopnode1</code></p>
<div class="highlighter-rouge"><pre class="highlight"><code>ApplicationMaster for attempt appattempt_1492900593461_0002_000001 timed out
</code></pre>
</div>
<p>Once again, it seems that insufficient resources are to blame. The container could not start properly.</p>

<h5 id="test-job-2">Test Job 2:</h5>
<p>Success! The job only took 1 min 30 sec.</p>
<div class="highlighter-rouge"><pre class="highlight"><code>17/04/23 07:38:31 INFO client.RMProxy: Connecting to ResourceManager at hadoopnode1/192.168.0.110:8032
17/04/23 07:38:35 INFO input.FileInputFormat: Total input paths to process : 1
17/04/23 07:38:36 INFO mapreduce.JobSubmitter: number of splits:1
17/04/23 07:38:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1492900593461_0003
17/04/23 07:38:38 INFO impl.YarnClientImpl: Submitted application application_1492900593461_0003
17/04/23 07:38:38 INFO mapreduce.Job: The url to track the job: http://hadoopnode1:8088/proxy/application_1492900593461_0003/
17/04/23 07:38:38 INFO mapreduce.Job: Running job: job_1492900593461_0003
17/04/23 07:39:18 INFO mapreduce.Job: Job job_1492900593461_0003 running in uber mode : false
17/04/23 07:39:18 INFO mapreduce.Job:  map 0% reduce 0%
17/04/23 07:39:41 INFO mapreduce.Job:  map 67% reduce 0%
17/04/23 07:39:44 INFO mapreduce.Job:  map 100% reduce 0%
17/04/23 07:40:09 INFO mapreduce.Job:  map 100% reduce 100%
17/04/23 07:40:10 INFO mapreduce.Job: Job job_1492900593461_0003 completed successfully
17/04/23 07:40:11 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=349637
		FILE: Number of bytes written=937165
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1111292
		HDFS: Number of bytes written=257189
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=45734
		Total time spent by all reduces in occupied slots (ms)=62559
		Total time spent by all map tasks (ms)=22867
		Total time spent by all reduce tasks (ms)=20853
		Total vcore-milliseconds taken by all map tasks=22867
		Total vcore-milliseconds taken by all reduce tasks=20853
		Total megabyte-milliseconds taken by all map tasks=5853952
		Total megabyte-milliseconds taken by all reduce tasks=8007552
	Map-Reduce Framework
		Map input records=19150
		Map output records=184794
		Map output bytes=1819171
		Map output materialized bytes=349637
		Input split bytes=99
		Combine input records=184794
		Combine output records=23648
		Reduce input groups=23648
		Reduce shuffle bytes=349637
		Reduce input records=23648
		Reduce output records=23648
		Spilled Records=47296
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=1192
		CPU time spent (ms)=13260
		Physical memory (bytes) snapshot=220286976
		Virtual memory (bytes) snapshot=641531904
		Total committed heap usage (bytes)=132689920
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=1111193
	File Output Format Counters
		Bytes Written=257189
</code></pre>
</div>
<p>I am honestly not sure what caused the difference between this job’s success and the previous one’s failure. The only difference was the time of day (last night vs. this morning).</p>

<p>Whatever the cause, this time the job stayed within the memory allocation limits.</p>

<h4 id="troubleshooting-references">Troubleshooting References</h4>
<ol>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-on-a-vps">Rsync</a></li>
  <li><a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-3-7d114d35fdf1">Jason Carter’s blog</a></li>
  <li><a href="http://stackoverflow.com/questions/10666488/what-are-success-and-part-r-00000-files-in-hadoop/10666874#10666874">Understanding Hadoop Output</a></li>
  <li><a href="https://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/">HortonWorks Production Cluster Configuration</a></li>
  <li>Wordcount test files: <a href="http://www.gutenberg.org/">Project Gutenberg</a></li>
</ol>

<h2 id="final-configuration">Final Configuration</h2>

<ul>
  <li><code class="highlighter-rouge">OS</code> Armbian 5.25 Orangepione Debian jessie default</li>
  <li><code class="highlighter-rouge">Hadoop Version</code> Apache Hadoop 2.7.3</li>
  <li><code class="highlighter-rouge">$HADOOP_HOME=/opt/hadoop-2.7.3</code></li>
</ul>

<h4 id="hadoop-configuration-hadoop_conf_dir">Hadoop Configuration: <code class="highlighter-rouge">$HADOOP_CONF_DIR/</code></h4>

<ol>
  <li>
    <p><code class="highlighter-rouge">hadoop-env.sh</code>:</p>

    <div class="language-bash highlighter-rouge"><pre class="highlight"><code> <span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/opt/jdk1.8.0_121/
 <span class="c"># The maximum amount of heap to use, in MB. Default is 1000.</span>
 <span class="nb">export </span><span class="nv">HADOOP_HEAPSIZE</span><span class="o">=</span>128
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">core-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hdfs://hadoopnode1:53410<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">hdfs-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.blocksize<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>5242880<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>file:/hdfs/namenode<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.datanode.name.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>file:/hdfs/datanode<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">yarn-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>hadoopnode1<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.memory-mb<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
     <span class="nt">&lt;description&gt;</span>Whether virtual memory limits will be enforced for container$
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">mapred-site.xml</code>:</p>

    <div class="language-xml highlighter-rouge"><pre class="highlight"><code> <span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>mapreduce.map.memory.mb<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>256<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>mapreduce.reduce.memory.mb<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>384<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
   <span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
 <span class="nt">&lt;/configuration&gt;</span>
</code></pre>
    </div>
  </li>
</ol>

<h2 id="project-discussion">Project Discussion</h2>

<p><em>DISCLAIMER: I do not understand everything about physical and virtual memory management in operating systems, especially under Hadoop, YARN and MapReduce. However, I know that Hadoop was designed for large-scale analytics on machines with many times the resources available in my cluster. In some ways this project was a lost cause before it began. This cluster of Orange Pi Ones is not the environment Hadoop was designed for, indeed, the jobs that I ran in this tutorial could have been run easily on my client machine alone, even with an application written in an interpreted language. But this does not prevent us from improving the setup as much as we can!</em></p>

<h3 id="hadoop-application-failure-analysis">Hadoop Application Failure Analysis</h3>
<p>Over the course of this project, Hadoop distributed applications failed for 2 main reasons:</p>

<ol>
  <li>Application timed out while ApplicationMaster was allocating a container</li>
  <li>Containers ran beyond virtual memory limits</li>
  <li>Block could not be found in HDFS</li>
</ol>

<p>I think the “Block could not be found in HDFS” error was due to file corruption after a hard poweroff, so we can discount reason #3 for root cause analysis. HDFS seems to corrupt very easily, so be sure not to pull the plug on a running Hadoop application!</p>

<h4 id="root-cause-memory-shortage">Root Cause: memory shortage</h4>
<p>After doing some research, I think the first two errors are due to the same root cause: limited memory resources of the Orange Pi One (512 MB). My thought is that a container was allocated to a given node, and a certain map or reduce task was assigned to that container. During the creation of the container or during the execution of the task, the memory required greatly exceeded the available physical memory, resulting in large virtual memory usage. For example, jobs which failed because of killed containers reported:</p>

<ul>
  <li><strong>Physical:</strong> 16.7 MB / 128 MB</li>
  <li><strong>Virtual:</strong> 1.1 GB / 268.8 MB</li>
</ul>

<p>This heavy reliance on virtual memory usage caused nodes in use to become unresponsive, causing the application to fail: If virtual memory limits were enforced, the container was killed, or if they were not enforced, the application would become unresponsive, and Hadoop would fail the job.</p>

<p>The configuration resources I used as a guide were either designed for a <a href="https://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/">production system</a> with RAM on the order of 50 GB, or a <a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#YARN_and_MapReduce_memory_configuration_overview">Raspberry Pi cluster</a> with about double the memory per machine. I attempted to scale down the YARN container allocation, JVM heap size, and Map &amp; Reduce task allocations, but I now think that the extremely small allocations only contributed to the runaway virtual memory usage.</p>

<hr />

<h3 id="future-work">Future Work</h3>
<p>Future work should address the node resource bottleneck. One concern is distributing the workload among the nodes. On jobs which used only a single task at a time, only one node at a time was involved in the processing at a time. It would certainly seem that Hadoop should be able to allocate smaller map and reduce tasks so that they fit within the memory of the cluster node.</p>

<h4 id="hdfs-block-storage">HDFS Block Storage</h4>
<p>Trying to spread out the file over the HDFS in a more even manner would seem to be a helpful as well. Decreasing the block size from the default value as I have done here seems to be helpful, but not enough to prevent Hadoop from storing all blocks of a small file on one node. <a href="http://www.widriksson.com/raspberry-pi-hadoop-cluster/">Jonas Widriksson’s original Hadoop v1 tutorial</a> has some useful ideas in this regard.</p>

<h4 id="hadoop-memory-management-configuration">Hadoop Memory Management Configuration</h4>
<p>A cluster of two Orange Pi Ones does not have the resources available to provide stable MapReduce performance. It seemed that as long as the container was successfully created by the ApplicationMaster, the job had a good chance of completing. But many map or reduce tasks just consumed more resources than a single node had available to give while still being responsive, crashing the job.</p>

<p>With detailed analysis and tweaking of the YARN, MapReduce, and Operating system configuration, I believe that small jobs could run stably on my cluster. But the small pool of resources can only be stretched so thin before a node thrashes and takes the cluster down with it.</p>

<hr />

<h3 id="conclusions">Conclusions</h3>
<p>There are two main conclusions from this project.</p>

<h4 id="configuration-must-be-optimized">Configuration Must be Optimized</h4>

<blockquote>
  <p>A Hadoop cluster stands or falls with its configuration.</p>
</blockquote>

<p>First, Hadoop configuration is the most complex part of setting up a Hadoop cluster, but it also seems to be the most important. A cluster’s performance is entirely dependent on the efficiency of its configuration. In fact, calculating the optimal resource configuration will be required I did not count on needing to understand the software architecture of the Hadoop system to install and use a cluster. However, as it turns out, you must be at least familiar with it before you can touch the first configuration file.</p>

<h4 id="my-cluster-needs-more-memory">My Cluster Needs More Memory</h4>

<p>And second, this cluster needs more memory. Adding more nodes would be the one approach for increasing the memory resources of the cluster, but it would leave the memory per node at its present critically small value. Thus, for this approach, optimization of Hadoop’s memory usage at the container, application, and node level is crucial for use of the Orange Pi One as a Hadoop cluster node. <a href="https://hadoop.apache.org/docs/r2.7.3/">Hadoop documentation</a>, here I come!</p>

<p>Using machines with more onboard memory as nodes will increase the memory resources of the cluster. This approach is clearly more closely aligned with the design of Hadoop, and would most likely improve the stability of the cluster without much change in the configuration. From my reading in other tutorials, it seems that 1 GB, double the memory of the Orange Pi One (512 MB), is sufficient to run Hadoop 2 MapReduce Jobs with consistent success.</p>

<h4 id="references-2">References</h4>
<ol>
  <li><a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-3-7d114d35fdf1">Jason Carter’s Blog</a></li>
  <li><a href="http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/#YARN_and_MapReduce_memory_configuration_overview">Jonas Widriksson’s Blog</a></li>
  <li><a href="http://www.widriksson.com/raspberry-pi-hadoop-cluster/">Jonas Widriksson’s Original Hadoop v1 Tutorial</a></li>
</ol>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">Orange Pi Hadoop Cluster maintained by <a href="http://github.com/andrew-pyle">andrew-pyle</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    
  </body>
</html>
